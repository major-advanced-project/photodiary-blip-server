from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch
from PIL import Image
import os
from dotenv import load_dotenv

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
token = os.getenv("HUGGINGFACE_TOKEN")
if token is None:
    raise ValueError("HUGGINGFACE_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤:", device)

# 3. ëª¨ë¸ í´ëž˜ìŠ¤ ì •ì˜
class Blip2Model:
    def __init__(self):
        self.model_name = "Salesforce/blip2-flan-t5-xl"
        print(f"ðŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")

        self.processor = Blip2Processor.from_pretrained(self.model_name, token=token)
        self.model = Blip2ForConditionalGeneration.from_pretrained(
            self.model_name,
            token=token,
            torch_dtype=torch.float16 if device.type == "cuda" else torch.float32
        ).to(device)
        self.model.eval()

        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name} on {device}")

    def predict(self, image: Image.Image, prompt: str = None) -> str:
        if prompt is None:
            prompt = "What is shown in this image? Please describe it in detail."

        inputs = self.processor(images=image, text=prompt, return_tensors="pt")
        input_tensors = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            generated_ids = self.model.generate(
                **input_tensors,
                max_new_tokens=80,
                do_sample=False,
                num_beams=1,
                repetition_penalty=1.1
            )

        caption = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return caption.strip()

